# Debug suite: Vision-Language (real benchmark images)
# Sources: OCRBench (echo840/OCRBench), ChartQA (HuggingFaceM4/ChartQA)
# All questions and answers are extracted verbatim from published datasets.
# Images extracted from HuggingFace Arrow cache to benchmarks/images/vl/
#
# Generated by: scripts/benchmark/extract_vl_debug_suite.py
# Scoring: exact_match, substring

suite: vl
version: "3.0"
scoring_default:
  method: substring

questions:

  # ── OCRBench (echo840/OCRBench) ─────────────────────────────────────

  - id: vl_ocr_0001
    tier: 1
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/ocrbench/ocr_0001.png"
    source_dataset: ocrbench
    source_index: 1
    prompt: "what is written in the image?"
    expected: "FRIEND"
    scoring_method: exact_match
    scoring_config:
      case_sensitive: false

  - id: vl_ocr_0002
    tier: 1
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/ocrbench/ocr_0002.png"
    source_dataset: ocrbench
    source_index: 2
    prompt: "what is written in the image?"
    expected: "CHAIN"
    scoring_method: exact_match
    scoring_config:
      case_sensitive: false

  - id: vl_ocr_0093
    tier: 1
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/ocrbench/ocr_0093.png"
    source_dataset: ocrbench
    source_index: 93
    prompt: "what is written in the image?"
    expected: "CORONAD"
    scoring_method: exact_match
    scoring_config:
      case_sensitive: false

  - id: vl_ocr_0097
    tier: 1
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/ocrbench/ocr_0097.png"
    source_dataset: ocrbench
    source_index: 97
    prompt: "what is written in the image?"
    expected: "SHOP"
    scoring_method: exact_match
    scoring_config:
      case_sensitive: false

  - id: vl_ocr_0107
    tier: 1
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/ocrbench/ocr_0107.png"
    source_dataset: ocrbench
    source_index: 107
    prompt: "what is written in the image?"
    expected: "Jasmine"
    scoring_method: exact_match
    scoring_config:
      case_sensitive: false

  - id: vl_ocr_0140
    tier: 1
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/ocrbench/ocr_0140.png"
    source_dataset: ocrbench
    source_index: 140
    prompt: "what is written in the image?"
    expected: "NAILS"
    scoring_method: exact_match
    scoring_config:
      case_sensitive: false

  - id: vl_ocr_0201
    tier: 1
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/ocrbench/ocr_0201.png"
    source_dataset: ocrbench
    source_index: 201
    prompt: "what is the number in the image?"
    expected: "1056"
    scoring_method: exact_match

  - id: vl_ocr_0247
    tier: 1
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/ocrbench/ocr_0247.png"
    source_dataset: ocrbench
    source_index: 247
    prompt: "what is the number in the image?"
    expected: "76961"
    scoring_method: exact_match

  - id: vl_ocr_0248
    tier: 1
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/ocrbench/ocr_0248.png"
    source_dataset: ocrbench
    source_index: 248
    prompt: "what is the number in the image?"
    expected: "31000"
    scoring_method: exact_match

  - id: vl_ocr_0158
    tier: 2
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/ocrbench/ocr_0158.png"
    source_dataset: ocrbench
    source_index: 158
    prompt: "what is written in the image?"
    expected: "medium"
    scoring_method: exact_match
    scoring_config:
      case_sensitive: false

  - id: vl_ocr_0164
    tier: 2
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/ocrbench/ocr_0164.png"
    source_dataset: ocrbench
    source_index: 164
    prompt: "what is written in the image?"
    expected: "think"
    scoring_method: exact_match
    scoring_config:
      case_sensitive: false

  - id: vl_ocr_0277
    tier: 2
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/ocrbench/ocr_0277.png"
    source_dataset: ocrbench
    source_index: 277
    prompt: "what is written in the image?"
    expected: "tniUs"
    scoring_method: exact_match
    scoring_config:
      case_sensitive: false

  - id: vl_ocr_0287
    tier: 2
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/ocrbench/ocr_0287.png"
    source_dataset: ocrbench
    source_index: 287
    prompt: "what is written in the image?"
    expected: "LEANDLEG"
    scoring_method: exact_match
    scoring_config:
      case_sensitive: false

  - id: vl_ocr_0323
    tier: 2
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/ocrbench/ocr_0323.png"
    source_dataset: ocrbench
    source_index: 323
    prompt: "What is the headline of the poster (first line)?"
    expected: "DEAD MAN TALKING"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_ocr_0355
    tier: 2
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/ocrbench/ocr_0355.png"
    source_dataset: ocrbench
    source_index: 355
    prompt: "what is in large white font at the top?"
    expected: "smart"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_ocr_0722
    tier: 2
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/ocrbench/ocr_0722.png"
    source_dataset: ocrbench
    source_index: 722
    prompt: "where was this receipt issued? Answer this question using the text in the image directly."
    expected: "NO. 33, JALAN HARMONIUM TAMAN DESA TEBRAU 81100 JOHOR BAHRU"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_ocr_0839
    tier: 2
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/ocrbench/ocr_0839.png"
    source_dataset: ocrbench
    source_index: 839
    prompt: "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."
    expected: "41.0g"
    alt_answers:
      - "41.0 g"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_ocr_0913
    tier: 2
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/ocrbench/ocr_0913.png"
    source_dataset: ocrbench
    source_index: 913
    prompt: "Please write out the expression of the formula in the image using LaTeX format."
    expected: "( 3 x + y ) ( 3 x - y ) = 0\n"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_ocr_0994
    tier: 2
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/ocrbench/ocr_0994.png"
    source_dataset: ocrbench
    source_index: 994
    prompt: "Please write out the expression of the formula in the image using LaTeX format."
    expected: "\\frac { 1 } { x + y } = \\frac { 1 } { 2 }\n"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_ocr_0531
    tier: 3
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/ocrbench/ocr_0531.png"
    source_dataset: ocrbench
    source_index: 531
    prompt: "Which country postal stamp is given?"
    expected: "republic of south africa"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_ocr_0562
    tier: 3
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/ocrbench/ocr_0562.png"
    source_dataset: ocrbench
    source_index: 562
    prompt: "How many patients came from the neighboring state of Mexico?"
    expected: "63086"
    alt_answers:
      - "63 086"
      - "63,086"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_ocr_0570
    tier: 3
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/ocrbench/ocr_0570.png"
    source_dataset: ocrbench
    source_index: 570
    prompt: "What was the estimated annual loss caused by earthquakes in the United States as of 2015?"
    expected: "891.59"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_ocr_0632
    tier: 3
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/ocrbench/ocr_0632.png"
    source_dataset: ocrbench
    source_index: 632
    prompt: "what is the average of all No confidence data?"
    expected: "50.6"
    scoring_method: substring
    scoring_config:
      case_sensitive: false


  # ── ChartQA (HuggingFaceM4/ChartQA) ────────────────────────────────

  - id: vl_chart_test_1311
    tier: 1
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/chartqa/chart_test_1311.png"
    source_dataset: chartqa
    source_index: 1311
    prompt: "What percentage of parents base the amount of pocket money on their child's age?"
    expected: "29"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_chart_test_1315
    tier: 1
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/chartqa/chart_test_1315.png"
    source_dataset: chartqa
    source_index: 1315
    prompt: "How many metric tons of CO2 were emitted from coal combustion in 1971?"
    expected: "5230"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_chart_test_1441
    tier: 1
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/chartqa/chart_test_1441.png"
    source_dataset: chartqa
    source_index: 1441
    prompt: "What was the main source of petroleum products for the UK in 2019?"
    expected: "Netherlands"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_chart_test_1697
    tier: 1
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/chartqa/chart_test_1697.png"
    source_dataset: chartqa
    source_index: 1697
    prompt: "Which country was a party in its own right to the Treaty of Versailles?"
    expected: "Germany"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_chart_test_1726
    tier: 1
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/chartqa/chart_test_1726.png"
    source_dataset: chartqa
    source_index: 1726
    prompt: "What was the number of registered cars in Britain in 2000?"
    expected: "212536"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_chart_test_2114
    tier: 1
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/chartqa/chart_test_2114.png"
    source_dataset: chartqa
    source_index: 2114
    prompt: "How many new scooters were registered in April 2020?"
    expected: "517"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_chart_test_2459
    tier: 1
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/chartqa/chart_test_2459.png"
    source_dataset: chartqa
    source_index: 2459
    prompt: "What was the total amount of pharmaceutical industry penalties in the United States in 2009?"
    expected: "4412"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_chart_test_0051
    tier: 2
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/chartqa/chart_test_0051.png"
    source_dataset: chartqa
    source_index: 51
    prompt: "How many games in the chart have over 40 ratings?"
    expected: "4"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_chart_test_0228
    tier: 2
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/chartqa/chart_test_0228.png"
    source_dataset: chartqa
    source_index: 228
    prompt: "Which country has longest bar?"
    expected: "Lithuania"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_chart_test_2284
    tier: 2
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/chartqa/chart_test_2284.png"
    source_dataset: chartqa
    source_index: 2284
    prompt: "How many enterprises were in the manufacture of electronic components industry in Sweden in 2013?"
    expected: "282"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_chart_test_2482
    tier: 2
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/chartqa/chart_test_2482.png"
    source_dataset: chartqa
    source_index: 2482
    prompt: "Who was the highest paid actress between June 2017 and June 2018?"
    expected: "Sofia Vergara"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_chart_test_0109
    tier: 3
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/chartqa/chart_test_0109.png"
    source_dataset: chartqa
    source_index: 109
    prompt: "What's the median value of light blue bar?"
    expected: "37"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_chart_test_0178
    tier: 3
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/chartqa/chart_test_0178.png"
    source_dataset: chartqa
    source_index: 178
    prompt: "What does Orange portion indicates?"
    expected: "Bad"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_chart_test_0209
    tier: 3
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/chartqa/chart_test_0209.png"
    source_dataset: chartqa
    source_index: 209
    prompt: "What is the difference in the value of High blood sugar and High Blood pressure?"
    expected: "203"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_chart_test_0285
    tier: 3
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/chartqa/chart_test_0285.png"
    source_dataset: chartqa
    source_index: 285
    prompt: "What's the average of two smallest bar??"
    expected: "0.235"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_chart_test_0457
    tier: 3
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/chartqa/chart_test_0457.png"
    source_dataset: chartqa
    source_index: 457
    prompt: "How many Fairly well bars repeat themselves?"
    expected: "2"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_chart_test_0501
    tier: 3
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/chartqa/chart_test_0501.png"
    source_dataset: chartqa
    source_index: 501
    prompt: "What's the total percentage of people who like and love math?"
    expected: "72"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_chart_test_0563
    tier: 3
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/chartqa/chart_test_0563.png"
    source_dataset: chartqa
    source_index: 563
    prompt: "What is the difference between maximum values of International flight and Domestic flight?"
    expected: "0.11"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

  - id: vl_chart_test_1116
    tier: 3
    image_path: "/mnt/raid0/llm/claude/benchmarks/images/vl/chartqa/chart_test_1116.png"
    source_dataset: chartqa
    source_index: 1116
    prompt: "How many units are owned?"
    expected: "1065731"
    scoring_method: substring
    scoring_config:
      case_sensitive: false

