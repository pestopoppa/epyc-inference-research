# Chapter 10: Orchestration Architecture

## Introduction

This project uses a **hierarchical local-agent workflow** for production inference. The design insight: different tasks require different capabilities, but most tokens are generated by simple expansion, not complex reasoning.

**Core Philosophy**:
> *One model thinks. Many models work. Tools decide who is right.*

## Problem Statement

Not every LLM task is equally hard. Conversational reasoning, long-form synthesis, code generation, and knowledge management all have wildly different compute profiles -- yet running them through one monolithic model wastes latency on simple tasks and starves complex ones of capacity.

<details>
<summary>Detailed problem breakdown</summary>

Modern LLM workflows mix fundamentally different tasks:
- Conversational reasoning and tool use
- Long-form document synthesis
- Software engineering across languages
- Personal knowledge management

Running all tasks through a single monolithic model is inefficient:
- Latency is unnecessarily high for simple tasks
- Throughput is wasted on tasks that don't need deep reasoning
- Quality drifts when small changes affect global context

</details>

## Core Insight

The key realization is that reasoning, planning, and expansion are separable concerns. Each phase can be handled by the cheapest model capable of doing it correctly -- which mirrors speculative decoding at the system level, where a strong model sets the trajectory and cheap models propose artifacts in parallel.

<details>
<summary>Decomposition and system-level speculation</summary>

**Reasoning, planning, and expansion are separable.**

If we decompose work into:
1. **Planning / architecture** (low token count, high value)
2. **Expansion / implementation** (high token count, lower entropy)
3. **Verification** (tool-driven, deterministic)

...then each phase can be handled by the *cheapest model capable of doing it correctly*.

This mirrors speculative decoding at the *system level*:
- A strong model sets the trajectory
- Many cheap models propose artifacts in parallel
- Correctness is enforced by gates, not by "agreement"

</details>

## Agent Tiers

The system organizes models into tiers: a fast always-resident front door (Tier A), specialist models for code/ingestion/architecture (Tier B), cheap parallel workers (Tier C), and tiny draft/embedding models (Tier D). Requests flow through the front door and get routed or escalated based on complexity and gate failures.

<details>
<summary>Tier definitions and escalation flows</summary>

### Tier A - Front Door / Orchestrator

**Purpose**: Interactive chat, intent classification, task routing

- Emits: `TaskIR` (JSON)
- Must be: Low-latency, always resident
- **Model**: Qwen3-Coder-30B-A3B with expert reduction (41.55 t/s)

### Tier B - Specialists

| Role | Purpose | Model | Acceleration |
|------|---------|-------|--------------|
| **B1: Coder** | Code generation, refactors | Qwen2.5-Coder-32B | Speculative (K=24) → 33 t/s |
| **B2: Ingestion** | Long-context synthesis | Qwen3-Next-80B-A3B | MoE reduction only (SSM!) |
| **B3: Architect (General)** | System design, invariants | Qwen3-235B-A22B | MoE reduction → 6.75 t/s |
| **B4: Architect (Coding)** | Ultimate code escalation | Qwen3-Coder-480B-A35B | MoE3 only → 10.3 t/s |

**Note**: Qwen3-Coder-480B (271GB) has a BOS token mismatch that breaks all speculation methods. Use expert reduction (MoE3) only. Despite being only ~35B active parameters per token, it achieves 95% quality on coding benchmarks with MoE3.

### Detailed Escalation Flow

<details>
<summary>Code: full escalation ASCII diagrams</summary>

```
User Request
     │
     ▼
┌─────────────────────────────────────────────────────────────┐
│           FRONTDOOR (Tier A - Always Resident)              │
│                                                             │
│  Model: Qwen3-Coder-30B-A3B + MoE6                          │
│  Speed: 18.3 t/s | Quality: 90%                             │
│                                                             │
│  • Intent classification                                    │
│  • TaskIR emission (JSON)                                   │
│  • Result synthesis                                         │
│  • Interactive chat (short queries handled directly)        │
└─────────────────────────────────────────────────────────────┘
     │
     ├─────────────┬─────────────┬─────────────┬─────────────┐
     ▼             ▼             ▼             ▼             ▼
   CODE        THINKING      INGEST        MATH         VISION
```

#### CODE Escalation Path

```
┌─────────────────────────────────────────────────────────────┐
│  CODER PRIMARY (B1) — Port 8080                              │
│  Model: Qwen3-Coder-30B-A3B + MoE6                          │
│  Speed: 18 t/s | Quality: 90%                               │
│  Handles: Single-file changes, simple refactors             │
└─────────────────────────────────────────────────────────────┘
     │ [failure OR multi-file OR review needed]
     ▼
┌─────────────────────────────────────────────────────────────┐
│  CODER ESCALATION / WORKER_SUMMARIZE — Port 8081             │
│  Model: Qwen2.5-Coder-32B + spec K=24 + lookup              │
│  Speed: 39 t/s | Quality: 96%                               │
│  Handles: Code review, summarization, multi-file refactors  │
└─────────────────────────────────────────────────────────────┘
     │ [architectural decision needed]
     ▼
┌─────────────────────────────────────────────────────────────┐
│  ARCHITECT_GENERAL (B3) — Port 8083                          │
│  Model: Qwen3-235B-A22B + MoE4                              │
│  Speed: 6.75 t/s | Quality: high                            │
│  Handles: Multi-module design, API contracts, invariants    │
└─────────────────────────────────────────────────────────────┘
     │ [ultimate escalation]
     ▼
┌─────────────────────────────────────────────────────────────┐
│  ARCHITECT_CODING (B4) — Port 8084                           │
│  Model: Qwen3-Coder-480B-A35B + MoE3                        │
│  Speed: 10.3 t/s | Quality: 83%                             │
│  Handles: Hardest architecture, novel algorithms            │
│  ⚠️  NO speculation - BOS mismatch breaks all drafting     │
└─────────────────────────────────────────────────────────────┘
```

#### THINKING / INGESTION Escalation Path

```
┌─────────────────────────────────────────────────────────────┐
│  FRONTDOOR (Tier A) — Port 8080                              │
│  Model: Qwen3-Coder-30B-A3B + MoE6                          │
│  Speed: 18 t/s                                              │
│  Handles: Quick reasoning, simple chain-of-thought          │
│  (Qwen3 models have native <think> support)                 │
└─────────────────────────────────────────────────────────────┘
     │ [needs deeper reasoning OR large context]
     ▼
┌─────────────────────────────────────────────────────────────┐
│  INGESTION (B2) — Port 8085                                  │
│  Model: Qwen3-Next-80B-A3B + MoE4                           │
│  Speed: 6.3 t/s | 128K context                              │
│  Handles: Long-context synthesis, deep reasoning            │
│  ⚠️  SSM architecture - NO speculation/prompt lookup!       │
└─────────────────────────────────────────────────────────────┘
     │ [architectural / system-level reasoning needed]
     ▼
┌─────────────────────────────────────────────────────────────┐
│  ARCHITECT_GENERAL (B3) — Port 8083                          │
│  Model: Qwen3-235B-A22B + MoE4                              │
│  Speed: 6.75 t/s                                            │
│  Handles: System design, invariants, hardest reasoning      │
└─────────────────────────────────────────────────────────────┘
```

</details>

**Note**: Dedicated thinking models (Qwen3-4B-Thinking, DeepSeek-R1-Distill-Qwen-32B) were evaluated during benchmarking but are not deployed in the production stack. The Qwen3 family has native `<think>` tag support, so reasoning is handled by the existing frontdoor and specialist models.

**Escalation Triggers**:
- Gate failure (lint, typecheck, unit tests)
- Multi-file changes detected
- Repetition loop detected (entropy spike)
- Context size exceeds model limit
- Explicit `max_escalation_level` in TaskIR
- THINK_HARDER exhausted on penultimate retry (see [Pipeline Intelligence](#pipeline-intelligence-february-2026))

**Implementation** (as of 2026-02-07): The escalation loop is driven by `src/graph/` — a pydantic-graph `Graph` with 7 node classes. Each node encodes valid transitions in its Union return type (e.g., `CoderNode.run()` returns `CoderNode | ArchitectNode | End[TaskResult]`). The graph replaces the manual for-loop in `repl_executor.py` and wires MemRL anti-memory functions (see [Chapter 15](15-memrl-system.md) and [Chapter 18](18-escalation-and-routing.md)).

### Tier C - Workers (Parallel)

Workers are stateless, cheap, and many run concurrently. They handle file-level implementation, exploration, summarization, test writing, documentation, math, and vision tasks.

| Worker | Model | Port | Tier | Acceleration |
|--------|-------|------|------|--------------|
| explore / math | Qwen2.5-7B-Instruct-f16 | 8082 | HOT | Spec K=24 + lookup (44 t/s) |
| summarize | Qwen2.5-Coder-32B (shared with coder_escalation) | 8081 | HOT | Spec K=24 + lookup (39 t/s) |
| vision | Qwen2.5-VL-7B Q4_K_M + mmproj | 8086 | HOT | None (VL model, ~15 t/s) |
| vision_escalation | Qwen3-VL-30B-A3B Q4_K_M + mmproj | 8087 | HOT | MoE4 (~10 t/s) |
| fast_1, fast_2 | Qwen2.5-Coder-1.5B Q4_K_M | 8102, 8112 | WARM | None (burst capacity) |

**Note**: `worker_coder` is the active coding worker semantic role (backed by the fast worker pool). Legacy `worker_code` remains alias-only for compatibility. Fast workers spin up on demand when concurrent load exceeds 4 tasks, and idle-shutdown after 5 minutes.

### Tier D - Draft / Embedder

- Tiny model for speculative decoding (co-loaded with spec decode servers)
- Embedding server for episodic memory (MemRL)
- **Draft Model**: Qwen2.5-Coder-0.5B-Instruct Q8_0 (co-loaded on ports 8081, 8082)
- **Embedder**: BGE-large-en-v1.5 F16 (6 parallel instances, ports 8090-8095)
  - Probe-first architecture: health probe all servers, first responder gets request
  - 1024-dim embeddings with CLS pooling (standard BERT)
  - Hotswap: `orchestrator_stack.py reload embedders`

### Formalizers (Preprocessing)

Formalizers are specialized models that convert vague natural language into structured IR before routing to execution tiers. They are a preprocessing step, not part of the main execution flow -- think of them as a "compile" pass that turns fuzzy user intent into a concrete tool-call sequence.

<details>
<summary>Formalizer architecture and candidate models</summary>

```
User: "Download the file, parse it, and upload results to S3"
                    │
                    ▼
┌─────────────────────────────────────────────────────────────┐
│  FORMALIZER (Preprocessing)                                 │
│                                                             │
│  Input: Vague natural language task                         │
│  Output: FormalizationIR (structured tool sequence)         │
│                                                             │
│  Candidate Models:                                          │
│  • xLAM-2-1B-fc-r (~90 t/s, outperforms GPT-4o on BFCL)    │
│  • xLAM-1B-fc-r (older version)                            │
│  • NexusRaven-V2-13B (better at nested functions)          │
└─────────────────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────┐
│  FormalizationIR Output:                                    │
│  {                                                          │
│    "tool_sequence": [                                       │
│      {"step": 0, "tool": "download_file", "args": {...}},   │
│      {"step": 1, "tool": "parse_data", "depends_on": [0]},  │
│      {"step": 2, "tool": "upload_s3", "depends_on": [1]}    │
│    ]                                                        │
│  }                                                          │
└─────────────────────────────────────────────────────────────┘
                    │
                    ▼
              Frontdoor → Execution Tiers
```

**When to use formalizers**:
- `task_type == 'agentic'` with multiple tool calls
- `ambiguity_score > 0.7` (detected by frontdoor)
- User request involves multi-step workflows

**Why not use general models?**
- Formalizers are trained specifically for function/tool extraction
- 1B params = fast preprocessing (doesn't bottleneck)
- Higher accuracy on structured output than general instruction models

**Status**: Evaluation complete. Top performers: xLAM-2-1B (100%), MathSmith-8B (100%). See [Benchmark Results](../reference/benchmarks/RESULTS.md).

</details>

</details>

## TaskIR Schema

The front door emits a TaskIR JSON for every non-trivial request. This is the contract between the orchestrator and all downstream agents -- it specifies what to do, who should do it, and how to verify the result.

<details>
<summary>TaskIR JSON structure</summary>

<details>
<summary>Code: TaskIR example payload</summary>

```json
{
  "task_id": "uuid",
  "task_type": "code | doc | ingest | manage",
  "priority": "interactive | batch",
  "objective": "Implement error handling for registry loader",
  "inputs": ["paths", "urls", "notes"],
  "constraints": ["language", "performance", "security"],
  "agents": [{"tier": "B", "role": "coder"}],
  "plan": {
    "steps": [
      {"id": "S1", "actor": "coder", "action": "implement", "outputs": ["src/file.py"]},
      {"id": "S2", "actor": "worker", "action": "test", "outputs": ["tests/test_file.py"]}
    ]
  },
  "gates": ["schema", "format", "lint", "typecheck", "unit"],
  "definition_of_done": ["Tests pass", "Type hints complete"],
  "escalation": {"max_level": "B3", "on_second_failure": true}
}
```

</details>

</details>

## Verification Gates

Every artifact passes through a strict ordered gate sequence: schema validation, formatting/lint, typecheck/build, unit tests, integration tests, and security checks. On first failure the producing agent gets another shot; on second failure it escalates one tier up. No free-form retries allowed.

<details>
<summary>Gate sequence and failure handling</summary>

Every artifact must pass, in order:

1. **Schema validation** (IR / JSON / OpenAPI)
2. **Formatting & lint** (shfmt, ruff, mdformat)
3. **Typecheck / build** (pyright, tsc)
4. **Unit tests**
5. **Integration tests** (if applicable)
6. **Security checks** (if defined)

**Gate failure handling**:
- First failure → return to producing agent
- Second failure → escalate one tier
- No free-form retries

</details>

## Routing Rules

Routing is deterministic and minimal. Short prompts that need no tools bypass the REPL entirely (direct-answer mode), while longer or more complex requests get routed to the appropriate tier based on task type, context size, and failure count.

<details>
<summary>Routing table and direct-answer mode</summary>

| Condition | Route |
|-----------|-------|
| Short prompt, no tools needed | **Direct-answer mode** (bypass REPL) |
| Short interactive query | Tier A only |
| Code generation | Tier B1 + workers |
| Large context (>20K chars) | Two-stage pipeline (B2 digest → A synthesis) |
| Vision / image analysis | VL worker (8086) → VL escalation (8087) |
| Architectural ambiguity | Tier B3 |
| Two gate failures | Escalate |

### Direct-Answer Mode

Short, simple prompts bypass the REPL Python-code wrapper entirely. The REPL forces the model to generate Python code + call `FINAL(answer)`, adding ~900 tokens of overhead. This destroys quality on instruction-precision tasks (same model: 11/11 without REPL, 2/11 through REPL).

Direct mode activates when the prompt has no file/tool operation keywords and context < 20K characters. The user prompt is sent directly to the model via `primitives.llm_call()` with no REPL wrapper. MemRL quality review gate still applies.

Implementation: `_should_use_direct_mode()` in `src/api/routes/chat.py`.

</details>

## Two-Stage Long Context Pipeline

When context exceeds 20K characters, the system splits it into chunks and fans out to parallel 7B workers for digesting, then the 30B front door synthesizes a final answer from all worker digests. This replaced the REPL exploration approach which scored 0/9 on long-context benchmarks.

<details>
<summary>Pipeline stages and rationale</summary>

All requests with context >20K characters use a two-stage pipeline instead of REPL exploration:

1. **Worker Parallel Digest** (7B at 44 t/s): Context split into N chunks (~4K tokens), each sent to a worker in parallel. Workers produce structured digests with key facts and task-relevant findings.
2. **Frontdoor Synthesis** (30B at 18 t/s): Receives all worker digests + original question, synthesizes final answer. No REPL code generation — text-in, text-out.

This replaced the REPL exploration approach (which scored 0/9 on long-context benchmarks because the model generated standalone Python instead of calling built-in `peek()`/`grep()` functions).

</details>

## Role Aliases

Models sometimes generate natural-language role names in `delegate()` and `escalate()` calls, so the system maps them automatically to canonical role identifiers.

<details>
<summary>Alias mapping table</summary>

| Model Generates | Maps To |
|----------------|---------|
| `researcher_agent` | `worker_explore` |
| `coder_agent` | `coder_escalation` |
| `reviewer_agent` | `architect_general` |
| `math_agent` | `worker_math` |
| `vision_agent` | `worker_vision` |
| `summarizer_agent` | `worker_summarize` |

</details>

## Key Design Principles

1. **Artifacts over prose**: Agents emit IR, code, tests, or diffs - not discussion
2. **Contracts first**: APIs and schemas constrain expansion
3. **Verification gates** replace consensus
4. **Explicit routing** prevents over-thinking

## Pipeline Intelligence (February 2026)

Three optimizations that improve speed and quality without changing the core graph structure. Each wraps around the existing escalation flow: a speculative pre-filter that tries cheap models first, a think-harder mechanism that squeezes more out of a model before escalating, and streaming tool use for real-time visibility.

<details>
<summary>Pre-filter, think-harder, and streaming details</summary>

### Try-Cheap-First Speculative Pre-Filter

Before the normal pipeline, a speculative attempt with the 7B worker model (44 t/s) is tried. If it passes the quality gate, the response is returned at 2-3x normal speed. On failure, the result is discarded and the request falls through to the normal pipeline with no penalty.

<details>
<summary>Code: pre-filter flow diagram</summary>

```
Request
   │
   ▼
┌─────────────────────────────────────────────────────────────┐
│  SPECULATIVE PRE-FILTER (worker_explore, 7B @ 44 t/s)      │
│                                                             │
│  • Single-turn attempt, no REPL, no escalation              │
│  • Quality gate: MemRL Q-score + length/coherence checks    │
│  • Pass → return immediately (2-3x faster)                  │
│  • Fail → discard, fall through to normal pipeline          │
└─────────────────────────────────────────────────────────────┘
   │ [quality gate failed]
   ▼
Normal Pipeline (frontdoor → coder → architect)
```

</details>

**Rollout phases**:

| Phase | Gate Logic | Purpose |
|-------|-----------|---------|
| **A** | All requests attempted | Bootstrap Q-value data for MemRL |
| **B** | MemRL Q-value gated | Only attempt when Q-score predicts success |
| **C** | Fully learned | Adaptive threshold from accumulated rewards |

**Parallel in Claude's stack**: This mirrors Anthropic's Haiku -> Sonnet -> Opus tier routing, where cheap models handle easy requests and expensive models are reserved for hard ones. The key difference is that our pre-filter runs speculatively (try-and-verify) rather than classifying upfront.

Feature flag: `try_cheap_first`.

### Think-Harder Escalation

New `THINK_HARDER` action fires on the penultimate retry (one attempt before model escalation). Instead of immediately escalating to a more expensive model, the same model is re-invoked with an enhanced configuration:

- **2x token budget** (doubled `max_tokens`)
- **Chain-of-thought prefix** injected into the system prompt
- **Slightly higher temperature** (0.7 -> 0.85) for more diverse reasoning

This often matches the quality of the next-tier model at 6x the speed. The action is inserted into the retry logic in each graph node: when `consecutive_failures == max_retries - 1` and the error category is `CODE` or `LOGIC`, `THINK_HARDER` fires instead of a normal retry.

<details>
<summary>Code: retry sequence with THINK_HARDER</summary>

```
Retry sequence (max_retries=3):
  Attempt 1 → fail → retry (same config)
  Attempt 2 → fail → THINK_HARDER (2x budget, CoT, higher temp)
  Attempt 3 → fail → escalate to next tier
```

</details>

**Parallel in Claude's stack**: Analogous to Claude's extended thinking (`budget_tokens`), where the same model is given more compute before falling back to a different approach.

Feature flag: `think_harder`.

### Streaming Tool Use

Token-level SSE (Server-Sent Events) streaming during REPL execution, providing real-time visibility into LLM generation and tool invocations.

**New SSE event types**:

| Event | Payload | When |
|-------|---------|------|
| `tool_start_event` | `{tool_name, args_preview}` | Tool registry dispatches a tool call |
| `tool_end_event` | `{tool_name, elapsed_ms, success}` | Tool execution completes |
| `token` | `{text, role}` | Each generated token during `llm_call_stream()` |

**Implementation**: `llm_call_stream()` method on `LLMPrimitives` yields tokens incrementally via the llama.cpp `/completion` endpoint with `stream: true`. The chat pipeline wraps this in SSE frames (`text/event-stream`) so the TUI and API clients can display partial output. Tool events are emitted by the `ToolRegistry` dispatch hooks added alongside the existing `InvocationRecord` logging.

**Integration point**: The REPL executor (`src/api/routes/chat_pipeline/repl_executor.py`) can optionally yield SSE events instead of buffering the full response. Callers opt in via `Accept: text/event-stream` header.

Feature flag: `streaming_tool_use`.

</details>

## Cross-Cutting Concerns (February 2026)

Eight concepts from OpenClaw and Lobster were integrated behind feature flags. Most remain default-off in production pending targeted validation; `session_compaction`, `tool_result_clearing`, and `depth_model_overrides` are now default-on with explicit guardrails and rollback toggles.

<details>
<summary>Concept-integration feature flags</summary>

### Content-Addressable LLM Cache

SHA-256 keyed prompt-to-response cache (`src/llm_cache.py`). JSON files on disk with TTL (168h default) and LRU eviction (10K max entries). Cache key includes prompt, role, token count, and model hash. Checked before `_real_call_single()` in inference, stored after success. Highest-value targets: `architect_general` (6.75 t/s), `ingest_long_context` (6.3 t/s).

Feature flag: `content_cache`.

### Model Fallback

When a backend is circuit-open or times out, same-tier alternatives are tried before failing. Distinct from task escalation (which handles complexity, not infrastructure). Fallback map in `src/roles.py`:

| Primary | Fallbacks |
|---------|-----------|
| architect_general | architect_coding, coder_escalation |
| architect_coding | architect_general, coder_escalation |
| coder_escalation | architect_coding |
| worker_math | worker_general |
| ingest_long_context | architect_general |
| frontdoor | (none) |
| worker_vision | (none) |

`BackendHealthTracker.classify_failure()` maps error messages to `FailoverReason` (circuit_open, timeout, connection_error, oom). Feature flag: `model_fallback`.

### Session Compaction (Virtual Memory Pattern)

Two-stage context pressure management in `src/graph/helpers.py`:

1. **C3: Tool Result Clearing** (40% of max_context) — regex-based clearing of stale `<<<TOOL_OUTPUT>>>...<<<END_TOOL_OUTPUT>>>` blocks from `state.last_output`. Keeps last N blocks (default 2), replaces older with `[Tool result cleared]`. Feature flag: `tool_result_clearing`.

2. **C1: Context Externalization** (60% of max_context) — "virtual memory" pattern instead of lossy summarization:
   - Dumps full verbatim `state.context` to a writable tmp path (`ORCHESTRATOR_PATHS_TMP_DIR` / configured tmp / `/mnt/raid0/llm/claude/tmp` / system tmp fallback) (zero information loss)
   - Generates a structured index via `worker_explore` (7B, not SERIAL_ROLES gated) using hot-swappable prompt (`orchestration/prompts/compaction_index.md`) with line coordinates for one-shot `read_file(path, offset=N, limit=M)` retrieval
   - If index generation fails (timeout/contention), compaction still proceeds using a deterministic fallback index
   - Index prompt now generates a **"Current Execution State"** block as the first section (what the system is working on, key values, next action) — inspired by the Markovian property from Delethink (arXiv:2510.06557)
   - Keeps recent context verbatim (configurable ratio, default 20%, min 3000 chars)
   - Replaces context with: `[Context Index]\n{index}\n\n[Recent Context]\n{verbatim}\n\nFull context: read_file("{path}")`
   - `compaction_count`, `compaction_tokens_saved`, `context_file_paths`, `last_compaction_turn` tracked in `TaskState`
   - Minimum-turn guard is configurable via `ORCHESTRATOR_CHAT_SESSION_COMPACTION_MIN_TURNS` (default 5)
   - Optional turn-based recompaction: when `session_compaction_recompaction_interval > 0`, re-triggers compaction every N turns after first compaction (prevents context regrowth)

**Design Rationale**:
- **Virtual memory over lossy summarization**: Delethink (arXiv:2510.06557) demonstrates that aggressive compression (100-token carryover) works when models are RL-trained for Markovian behavior. Our models aren't RL-trained, so we use lossless externalization with `read_file()` fallback to compensate — any information lost from the active window can be paged back in on demand.
- **"Current Execution State" leads the index**: The Markovian property (future steps depend primarily on recent state) means the most valuable information after compaction is *what the model was doing*, not *what happened historically*. The execution state block captures active task, key values, and next action.
- **20% default retention with configurable ratio**: Delethink uses <2% with RL training. We default to 20% as a conservative choice for untrained models. Configurable via `ORCHESTRATOR_CHAT_SESSION_COMPACTION_KEEP_RECENT_RATIO` for future tuning as index quality improves.

Token counting: `LlamaTokenizer` (`src/llm_primitives/tokenizer.py`) calls llama-server `/tokenize` with MD5-keyed LRU cache and 500ms timeout, falling back to `len//4`. Feature flag: `accurate_token_counting`.

Related persistence path (Phase 3): when `/chat` receives `session_id`, REPL checkpoints can restore JSON-safe `user_globals` from the latest session checkpoint before graph execution. This extends session continuity beyond in-request turn sharing.

### Resume Tokens

Base64url-encoded graph state for crash recovery (`src/graph/resume_token.py`). Contents: task_id, node_class, current_role, turns, escalation/failure counts, role_history, last_error (truncated 200 chars), SHA-256[:8] checksum. Encoding: JSON -> zlib -> base64url (<500 bytes). Generated in `snapshot_node()`, decoded in `run_task()`. Feature flag: `resume_tokens`.

### Approval Gates

Human approval at escalation boundaries and destructive tool invocations (`src/graph/approval_gate.py`). Three halt triggers: tier crossing (ESCALATION), destructive tools (DESTRUCTIVE_TOOL), architect-tier models (HIGH_COST). Protocol: `should_halt()` -> build `HaltState` -> call `ApprovalCallback.request_approval()` -> APPROVE or REJECT. Default `AutoApproveCallback` preserves current behavior. Feature flag: `approval_gates` (requires `resume_tokens` + `side_effect_tracking`).

### Binding-Based Routing

Priority-ordered routing overrides (`src/routing_bindings.py`). Five priority levels: DEFAULT (0) < CLASSIFIER (10) < Q_VALUE (20) < USER_PREF (30) < SESSION (40). `BindingRouter.resolve()` returns highest-priority active binding for a task type. Session bindings cleared at conversation end. Integrated in `_classify_and_route()`. Feature flag: `binding_routing`.

</details>

### Safe-Default Review (R6, 2026-02)

- Enabled by default now:
  - `session_compaction`
    - Benefit: bounds prompt growth in long-running sessions and reduces downstream latency/timeout pressure.
    - Rollback: `ORCHESTRATOR_SESSION_COMPACTION=0`.
    - Smoke checks: long-context `/chat` run keeps answer quality stable and reports compaction activity without restore regressions.
  - `tool_result_clearing`
    - Benefit: clears stale tool output blocks early to reduce context pressure before compaction.
    - Rollback: `ORCHESTRATOR_TOOL_RESULT_CLEARING=0`.
    - Smoke checks: tool-heavy REPL turns show non-zero `tool_results_cleared` without answer regressions.
  - `depth_model_overrides`
    - Benefit: routes nested `llm_call` recursion to cheaper worker-tier roles to reduce delegated latency/cost during deep tool/delegation loops.
    - Guardrails: worker-only targets + max-depth cap (`LLMConfig.depth_override_max_depth`, default `3`) + backend-availability fallback.
    - Rollback: `ORCHESTRATOR_DEPTH_MODEL_OVERRIDES=0` (or role-map override via `ORCHESTRATOR_LLM_DEPTH_ROLE_OVERRIDES`).
    - Smoke checks: confirm `budget_diagnostics.depth_override_events` increments on nested calls, with no increase in timeout or stale-lock symptoms.
- Kept off by default (higher-risk or not yet fully validated):
  - `content_cache` (`ORCHESTRATOR_CONTENT_CACHE=1`).
  - `model_fallback` (`ORCHESTRATOR_MODEL_FALLBACK=1`).
  - `structured_tool_output` (`ORCHESTRATOR_STRUCTURED_TOOL_OUTPUT=1`).
  - `side_effect_tracking` + `approval_gates` (`ORCHESTRATOR_SIDE_EFFECT_TRACKING=1`, `ORCHESTRATOR_APPROVAL_GATES=1`).

### R3 + Phase 6 Validation Closure (2026-02-19)

- R3 depth-override rollout tuning is closed for this cycle:
  - live ON/OFF delegated probes confirmed telemetry toggle behavior (`budget_diagnostics.depth_override_enabled`),
  - delegated execution stayed bounded with explicit diagnostics in both modes.
- Phase 6 (early failure detection) is now load-validated:
  - targeted unit/integration monitor suite passed (`47 passed`),
  - concurrent live probe with monitor enabled produced bounded explicit outcomes (successes and explicit `504` timeouts), with no silent hangs.

## Implementation Status

The orchestration layer is spread across about 20 modules. The core dispatch, execution, and context-passing logic lives in `src/`, with prompt templates hot-swappable from `orchestration/prompts/`, and the full test suite covers 2841 tests with zero failures.

<details>
<summary>Module inventory and test coverage</summary>

The orchestration layer is implemented in:
- `src/dispatcher.py` - Task routing
- `src/executor.py` - Step execution, escalation
- `src/context_manager.py` - Context passing between steps
- `src/prompt_builders/` - Prompt construction package (7 sub-modules: types, constants, builder, review, code_utils, formatting, resolver). `build_corpus_context()` injects corpus-retrieved code snippets on turn 0 for coder roles (32B, 480B). Auto-inits from `model_registry.yaml` corpus config.
- `src/services/corpus_retrieval.py` - Singleton `CorpusRetriever` with n-gram + keyword fallback retrieval. App-layer concern (not llama-server flag). Enabled: 32B (+8.7pp accept), 480B (+15.6pp accept). Disabled: 7B (saturated), 30B/235B (net-negative).
- `orchestration/prompts/` - Hot-swappable prompt templates (18 .md files, read on every request via `resolve_prompt()`)
- `src/gate_runner.py` - Verification gate execution (sequential + parallel via `asyncio.gather()`)
- `src/features.py` - Feature flag system with dataclass validation (8 concept-integration flags + 3 pipeline intelligence flags)
- `src/config.py` - Centralized configuration (~185 values)
- `src/llm_cache.py` - Content-addressable LLM response cache
- `src/routing_bindings.py` - Priority-ordered routing bindings
- `src/graph/resume_token.py` - Base64url continuation tokens
- `src/graph/approval_gate.py` - Halt/resume approval protocol
- `orchestration/model_registry.yaml` - Deterministic model mapping
- `orchestration/task_ir.schema.json` - TaskIR validation
- `orchestration/repl_memory/` - MemRL episodic memory (staged rewards, FAISS retrieval, Q-scoring)

**Test Coverage**: 2841 tests passing (0 failures), including 83 concept-integration tests

</details>

## References

<details>
<summary>Literature and further reading</summary>

### Multi-Agent Systems and Orchestration

1. Wu, Q., Bansal, G., Zhang, J., Wu, Y., Li, B., Zhu, E., ... & Wang, C. (2023). *AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation*. arXiv preprint. https://arxiv.org/abs/2308.08155

2. Hong, S., Zhuge, M., Chen, J., Zheng, X., Cheng, Y., Zhang, C., ... & Wu, Y. (2024). *MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework*. ICLR 2024. https://arxiv.org/abs/2308.00352

3. Park, J. S., O'Brien, J. C., Cai, C. J., Morris, M. R., Liang, P., & Bernstein, M. S. (2023). *Generative Agents: Interactive Simulacra of Human Behavior*. UIST 2023. https://arxiv.org/abs/2304.03442

### Task Decomposition and Planning

4. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., ... & Zhou, D. (2022). *Chain-of-Thought Prompting Elicits Reasoning in Large Language Models*. NeurIPS 2022. https://arxiv.org/abs/2201.11903

5. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., & Narasimhan, K. (2024). *Tree of Thoughts: Deliberate Problem Solving with Large Language Models*. NeurIPS 2023. https://arxiv.org/abs/2305.10601

6. Khot, T., Trivedi, H., Finlayson, M., Fu, Y., Richardson, K., Clark, P., & Sabharwal, A. (2023). *Decomposed Prompting: A Modular Approach for Solving Complex Tasks*. ICLR 2023. https://arxiv.org/abs/2210.02406

### Verification and Self-Correction

7. Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., ... & Clark, P. (2024). *Self-Refine: Iterative Refinement with Self-Feedback*. NeurIPS 2023. https://arxiv.org/abs/2303.17651

8. Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., & Yao, S. (2024). *Reflexion: Language Agents with Verbal Reinforcement Learning*. NeurIPS 2023. https://arxiv.org/abs/2303.11366

### Code Generation and Software Engineering

9. Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., & Narasimhan, K. (2024). *SWE-bench: Can Language Models Resolve Real-World GitHub Issues?*. ICLR 2024. https://arxiv.org/abs/2310.06770

10. Yang, J., Jimenez, C. E., Wettig, A., Liber, K., Yao, S., Pei, K., ... & Narasimhan, K. (2024). *SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering*. arXiv preprint. https://arxiv.org/abs/2405.15793

### Hierarchical and Scalable Architectures

11. Shen, Y., Song, K., Tan, X., Li, D., Lu, W., & Zhuang, Y. (2024). *HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face*. NeurIPS 2023. https://arxiv.org/abs/2303.17580

12. Qiao, S., Ou, Y., Zhang, N., Chen, X., Yao, Y., Deng, S., ... & Chen, H. (2024). *Reasoning with Language Model Prompting: A Survey*. ACL 2023. https://arxiv.org/abs/2212.09597

</details>

---

*Previous: [Chapter 09: Deprecated Approaches](09-deprecated-approaches.md)* | *Next: [Chapter 11: REPL Environment](11-repl-environment.md)*

## Posterior Routing with Risk Controls (2026-02)

Routing now combines heuristic priors from the classifier with learned posterior evidence from MemRL retrieval, cost-aware ranking, and risk-controlled confidence thresholding. This gives the system a principled way to balance speed, cost, and quality when picking which model handles a request.

<details>
<summary>Routing signal sources and implementation</summary>

Routing now combines:

1. Heuristic priors from classifier/intent hints.
2. Learned posterior evidence from MemRL retrieval (robust Q-confidence via median/trimmed-mean top-k Q).
3. Cost-aware ranking using expected warm/cold latency (`selection_score`).
4. Risk-controlled confidence thresholding (`confidence_threshold` or calibrated threshold plus conformal margin).

Primary implementation paths:

- `src/api/routes/chat_pipeline/routing.py`
- `src/api/routes/chat_routing.py`
- `orchestration/repl_memory/retriever.py`

</details>
